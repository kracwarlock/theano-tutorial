{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is Theano?\n",
    "\n",
    "- Python library developed by machine learning researchers at the University of Montreal\n",
    "- define, optimize, and evaluate mathematical expressions involving multi-dimensional arrays efficiently\n",
    "- tight integration with NumPy - use numpy.ndarray in Theano-compiled functions\n",
    "- **efficient symbolic differentiation** - Theano does your derivatives for function with one or many inputs\n",
    "- supports both CPU and GPU\n",
    "- uses fast back-ends (CUDA, BLAS, dynamically generated C code)\n",
    "- supports CuDNN v3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f: [ 0.30044614  0.77454353  0.3901409 ]\n"
     ]
    }
   ],
   "source": [
    "import theano\n",
    "from theano import tensor\n",
    "import numpy\n",
    "numpy.random.seed(1234)\n",
    "theano.config.floatX = 'float64'\n",
    "\n",
    "# allows declaration of symbolic variables\n",
    "x = tensor.vector('x')\n",
    "W = tensor.matrix('W')\n",
    "b = tensor.vector('b')\n",
    "\n",
    "# function syntax is similar to NumPy\n",
    "xW  = tensor.dot(x, W)\n",
    "out = tensor.nnet.sigmoid(xW + b)\n",
    "\n",
    "# create a function variable which you can call to get output on given input\n",
    "f = theano.function(inputs=[x, W, b], outputs=out)\n",
    "\n",
    "# evaluate the above function for some input\n",
    "x_val = numpy.random.rand(4)           # 1 x 4\n",
    "W_val = numpy.random.randn(4, 3)       # 4 x 3\n",
    "b_val = numpy.random.rand(3)           # 1 x 3\n",
    "\n",
    "print 'f:', f(x_val, W_val, b_val)     # 1 x 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Strong Typing\n",
    "- all Theano variables have a type\n",
    "- Examples\n",
    "  - TensorType for NumPy ndarrays\n",
    "  - CudaNdarrayType for CUDA arrays\n",
    "  - Sparse for scipy.sparse matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<TensorType(float32, matrix)>\n"
     ]
    }
   ],
   "source": [
    "# create theano variables using theano.shared()\n",
    "theano.config.floatX = 'float32'\n",
    "tvar = theano.shared(numpy.array([[1, 2], [3, 4]], dtype=theano.config.floatX))\n",
    "print tvar.type()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 3.  4.]\n",
      " [ 2.  1.]]\n"
     ]
    }
   ],
   "source": [
    "# Set and get variable value\n",
    "tvar.set_value(numpy.array([[3, 4], [2, 1]], dtype=theano.config.floatX))\n",
    "print tvar.get_value()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<TensorType(float32, matrix)>\n"
     ]
    }
   ],
   "source": [
    "print tvar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.0\n"
     ]
    }
   ],
   "source": [
    "a = theano.shared(1)\n",
    "b = tensor.scalar()\n",
    "add = a + b\n",
    "adder = theano.function(inputs=[b], outputs=add)\n",
    "print adder(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise caution when using Python keywords\n",
    "Theano does not redefine Python keywords for Theano shared variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n"
     ]
    }
   ],
   "source": [
    "var = 0\n",
    "if var:\n",
    "    print 'True'\n",
    "else:\n",
    "    print 'False'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "var = theano.shared(var)\n",
    "if var:\n",
    "    print 'True'\n",
    "else:\n",
    "    print 'False'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n"
     ]
    }
   ],
   "source": [
    "if var.get_value():\n",
    "    print 'True'\n",
    "else:\n",
    "    print 'False'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- If you need an if-else expr in your symbolic graph use `theano.ifelse.ifelse()`\n",
    "- Similary `for i in var:` will not work; Use `theano.scan()` for looping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using the GPU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- default device can be specified in the .theanorc file\n",
    "- use float32 when using most GPUs\n",
    "- floatX can be specified as a command line flag\n",
    "\n",
    "For CPU:\n",
    "```\n",
    "THEANO_FLAGS=device=cpu,floatX=float32 python your_script.py\n",
    "```\n",
    "For GPU:\n",
    "```\n",
    "THEANO_FLAGS=device=gpu0,floatX=float32 python your_script.py\n",
    "```\n",
    "To obtain the current config parameters:\n",
    "```\n",
    "print theano.config.device\n",
    "print theano.config.floatX\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradients\n",
    "#### Theano has graph-based automatic symbolic differentiation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "x = tensor.vector('x')\n",
    "W = tensor.matrix('W')\n",
    "b = tensor.vector('b')\n",
    "y = tensor.vector('y')\n",
    "\n",
    "xW = tensor.dot(x,W)\n",
    "out = tensor.nnet.sigmoid(xW + b)\n",
    "\n",
    "C = ((out - y) ** 2).sum()\n",
    "\n",
    "# compute gradients w.r.t. parameters\n",
    "dC_dW, dC_db = theano.grad(C, wrt=[W, b])\n",
    "cost_and_grads = theano.function(inputs=[x, W, b, y], outputs=[C, dC_dW, dC_db])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\n",
      "0.968002200127\n",
      "dC/dW:\n",
      "[[ 0.14226542  0.12770246  0.15071078]\n",
      " [ 0.01742641  0.01564256  0.0184609 ]\n",
      " [ 0.08526361  0.07653563  0.09032514]\n",
      " [ 0.21572049  0.19363831  0.22852637]]\n",
      "dC/db:\n",
      "[ 0.23117696  0.20751259  0.24490039]\n"
     ]
    }
   ],
   "source": [
    "x_val = numpy.random.rand(4).astype('float32')\n",
    "W_val = numpy.random.randn(4, 3).astype('float32')\n",
    "b_val = numpy.random.rand(3).astype('float32')\n",
    "y_val = numpy.random.uniform(size=3).astype('float32')\n",
    "\n",
    "C_val, dC_dW_val, dC_db_val = cost_and_grads(x_val, W_val, b_val, y_val)\n",
    "print 'C:\\n',C_val\n",
    "print 'dC/dW:\\n',dC_dW_val\n",
    "print 'dC/db:\\n',dC_db_val"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Looping - theano.scan()\n",
    "\n",
    "### `sequences` - variables that `scan()` should iterate over as it loops"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suppose we wish to compute the element-wise product of two vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "vector1 = tensor.vector('vector1')\n",
    "vector2 = tensor.vector('vector2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "output, updates = theano.scan(fn=lambda a, b : a * b,\n",
    "                              sequences=[vector1, vector2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first iteration will take as input the first element of every sequence, the second iteration will take as input the second element of every sequence, etc. These individual element have will have one less dimension than the original sequences. For example, for a matrix sequence, the individual elements will be vectors.\n",
    "\n",
    "The parameter fn receives a function or lambda expression that expresses the computation to do at every iteration. It operates on the symbolic inputs to produce symbolic outputs. It will only ever be called once, to assemble the Theano graph used by Scan at every the iterations.\n",
    "\n",
    "Since we wish to iterate over both `vector1` and `vector2` simultaneously, we provide them as sequences. This means that every iteration will operate on two inputs: an element from `vector1` and the corresponding element from `vector2`.\n",
    "\n",
    "Because what we want is the elementwise product between the vectors, we provide a lambda expression that, given an element `a` from `vector1` and an element `b` from `vector2` computes and return the product."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "f = theano.function(inputs=[vector1, vector2],\n",
    "                    outputs=output,\n",
    "                    updates=updates)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calling `scan()`, we see that it returns two outputs.\n",
    "The first output **`output`** contains the outputs of fn from every timestep concatenated into a tensor. In our case, the output of a single timestep is a scalar so output is a vector where output[i] is the output of the i-th iteration.\n",
    "The second output **`updates`** details if and how the execution of the `scan()` updates any shared variable in the graph. It should be provided as an argument when compiling the Theano function.\n",
    "\n",
    "If updates is omitted, the state of any shared variables modified by `scan()` will not be updated properly. **Always provide updates when compiling your Theano function**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  0.   2.   6.  12.  20.]\n"
     ]
    }
   ],
   "source": [
    "vector1_value = numpy.arange(0, 5).astype(theano.config.floatX) # [0,1,2,3,4]\n",
    "vector2_value = numpy.arange(1, 6).astype(theano.config.floatX) # [1,2,3,4,5]\n",
    "print(f(vector1_value, vector2_value))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An interesting thing is that we never explicitly told `scan()` how many iteration it needed to run. It was automatically inferred; when given sequences, `scan()` will run as many iterations as the length of the shortest sequence."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `non_sequences` - variables required to be available 'as is' in each iteration of `scan()`\n",
    "- We do not want `scan()` to iterate over them and give only part of them at every iteration. We want the complete variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X = tensor.matrix('X')\n",
    "W = tensor.matrix('W')\n",
    "b = tensor.vector('b')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To have the full weight matrix `W` and the full bias vector `b` available at every iteration, we use the argument `non_sequences`. \n",
    "- Every non-sequence is passed as input to every iteration\n",
    "- `fn` will have access to both `sequences` and `non_sequences`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def step(v, W, b):\n",
    "    return tensor.dot(v, W) + b\n",
    "\n",
    "output, updates = theano.scan(fn=step,\n",
    "                              sequences=[X],\n",
    "                              non_sequences=[W, b])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/shikhar/.local/lib/python2.7/site-packages/theano/scan_module/scan_perform_ext.py:135: RuntimeWarning: numpy.ndarray size changed, may indicate binary incompatibility\n",
      "  from scan_perform.scan_perform import *\n"
     ]
    }
   ],
   "source": [
    "f = theano.function(inputs=[X, W, b],\n",
    "                    outputs=output,\n",
    "                    updates=updates)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The inputs that correspond to the non-sequences are always last and in the same order as the non-sequences are provided to `scan()`.\n",
    "- `v` : individual element of the sequence `X`\n",
    "- `W` and `b` : non-sequences `W` and `b`, respectively"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_value:\n",
      "[[-3. -2.]\n",
      " [-1.  0.]\n",
      " [ 1.  2.]]\n",
      "W_value:\n",
      "[[ 1.  0.]\n",
      " [ 0.  1.]]\n",
      "b_value:\n",
      "[ 0.  1.]\n",
      "f:\n",
      "[[-3. -1.]\n",
      " [-1.  1.]\n",
      " [ 1.  3.]]\n"
     ]
    }
   ],
   "source": [
    "X_value = numpy.arange(-3, 3).reshape(3, 2).astype(theano.config.floatX)\n",
    "W_value = numpy.eye(2).astype(theano.config.floatX)\n",
    "b_value = numpy.arange(2).astype(theano.config.floatX)\n",
    "print 'X_value:\\n', X_value\n",
    "print 'W_value:\\n', W_value\n",
    "print 'b_value:\\n', b_value\n",
    "print 'f:\\n',(f(X_value, W_value, b_value))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### theano.scan() - reusing outputs from the previous iterations\n",
    "In this example, we will use Scan to compute a cumulative sum over the first dimension of a matrix M. This means that the output will be a matrix S in which the first row will be equal to the first row of M, the second row will be equal to the sum of the two first rows of M, and so on.\n",
    "\n",
    "Another way to express this, which is the way we will implement here, is that S[t]=S[t−1]+M[t]. Implementing this with Scan would involve iterating over the rows of the matrix M and, at every iteration, reuse the cumulative row that was output at the previous iteration and return the sum of it and the current row of M.\n",
    "\n",
    "If we assume for a moment that we can get Scan to provide the output value from the previous iteration as an input for every iteration, implementing a step function is simple:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def step(m_row, cumulative_sum):\n",
    "    return m_row + cumulative_sum"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The trick part is informing Scan that our step function expects as input the output of a previous iteration. To achieve this, we need to use a new parameter of the scan() function: outputs_info. This parameter is used to tell Scan how we intend to use each of the outputs that are computed at each iteration.\n",
    "\n",
    "- For a non-recurrent output (like in every example before this one), the element should be None.\n",
    "- For a simple recurrent output (iteration t depends on the value at iteration t−1), the element must be a tensor. Scan will interpret it as being an initial state for a recurrent output and give it as input to the first iteration, pretending it is the output value from a previous iteration. For subsequent iterations, Scan will automatically handle giving the previous output value as an input.\n",
    "\n",
    "The step() function needs to expect one additional input for each simple recurrent output. These inputs correspond to outputs from previous iteration and are always after the inputs that correspond to sequences but before those that correspond to non-sequences. The are received by the step() function in the order in which the recurrent outputs are declared in the outputs_info sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "M = tensor.matrix('X')\n",
    "s = tensor.vector('s') # Initial value for the cumulative sum\n",
    "\n",
    "output, updates = theano.scan(fn=step,\n",
    "                              sequences=[M],\n",
    "                              outputs_info=[s])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now compile and test the Theano function :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  0.   1.   2.]\n",
      " [  3.   5.   7.]\n",
      " [  9.  12.  15.]]\n"
     ]
    }
   ],
   "source": [
    "f = theano.function(inputs=[M, s],\n",
    "                    outputs=output,\n",
    "                    updates=updates)\n",
    "\n",
    "M_value = numpy.arange(9).reshape(3, 3).astype(theano.config.floatX)\n",
    "s_value = numpy.zeros((3, ), dtype=theano.config.floatX)\n",
    "print(f(M_value, s_value))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### theano.scan() - reusing outputs from multiple past iterations\n",
    "The Fibonacci sequence : 1, 1, 2, 3, 5, 8, 13, ...\n",
    "F[n] = F[n-1] + F[n-2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def step(f_minus2, f_minus1):\n",
    "    new_f = f_minus2 + f_minus1\n",
    "    return new_f"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- for non-recurrent outputs, the value is None\n",
    "- for simple recurrent outputs, the value is a single initial state\n",
    "- for general recurrent outputs, where iteration t may depend on multiple past values, the value is a dictionary\n",
    "That dictionary has two values:\n",
    "- **taps** : list declaring which previous values of that output every iteration will need. [-2, -1] would mean every iteration should take as input the last 2 values of that output.\n",
    "- **initial** : tensor of initial values. If every initial value has n dimensions, initial will be a single tensor of n+1 dimensions with as many initial values as the oldest requested tap. In the case of the Fibonacci sequence, the individual initial values are scalars so the initial will be a vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "f_init = tensor.fvector()\n",
    "outputs_info = [dict(initial=f_init, taps=[-2, -1])]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have no sequence, we need to explicitly tell Scan how many iterations to run using the n_step parameter. The value can be real or symbolic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "output, updates = theano.scan(fn=step,\n",
    "                              outputs_info=outputs_info,\n",
    "                              n_steps=10)\n",
    "\n",
    "next_fibonacci_terms = output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's compile our Theano function which will take a vector of consecutive values from the Fibonacci sequence and compute the next 10 values :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[   2.    3.    5.    8.   13.   21.   34.   55.   89.  144.]\n"
     ]
    }
   ],
   "source": [
    "f = theano.function(inputs=[f_init],\n",
    "                    outputs=next_fibonacci_terms,\n",
    "                    updates=updates)\n",
    "\n",
    "out = f([1, 1])\n",
    "print(out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Psuedocode for softmax regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sgd(lr, tparams, grads, inp, cost):\n",
    "    gshared = [theano.shared(p.get_value() * numpy.float32(0.), name='%s_grad'%k) for k, p in tparams.iteritems()]\n",
    "    gsup = [(gs, g) for gs, g in zip(gshared, grads)]\n",
    "\n",
    "    f_grad_shared = theano.function(inp, cost, updates=gsup, profile=False)\n",
    "\n",
    "    pup = [(p, p - lr * g) for p, g in zip(itemlist(tparams), gshared)]\n",
    "    f_update = theano.function([lr], [], updates=pup, profile=False)\n",
    "\n",
    "    return f_grad_shared, f_update\n",
    "\n",
    "def build_model(tparams, options):\n",
    "    trng = RandomStreams(1234)\n",
    "    use_noise = theano.shared(numpy.float32(0.))\n",
    "\n",
    "    x = tensor.matrix('x', dtype='float32')\n",
    "    y = tensor.vector('y', dtype='int64')\n",
    "    W = tparams['W']\n",
    "    b = tparams['b']\n",
    "\n",
    "    sm    = tensor.nnet.softmax(tensor.dot(x,W)+b)\n",
    "    cost  = -(tensor.log(sm[tensor.arange(y.shape[0]), y]+1e-8)).mean()\n",
    "    wdec  = W*W\n",
    "    cost  = cost + options['decay_c']*wdec.sum().sum()/2\n",
    "    on    = tensor.argmax(sm,axis=1)\n",
    "    pred  = tensor.eq(on,y)\n",
    "    pred  = pred.reshape([pred.shape[0],1])\n",
    "    \n",
    "    return trng, use_noise, x, y, pred, cost\n",
    "\n",
    "..............\n",
    "def main():\n",
    "    trng, use_noise, x, y, pred, cost = build_model(tparams, model_options)\n",
    "    grads                    = tensor.grad(cost, wrt=itemlist(tparams))\n",
    "    lr                       = tensor.scalar(name='lr')\n",
    "    f_grad_shared, f_update  = eval('sgd')(lr, tparams, grads, [x,y], cost)\n",
    "    f_preds                  = theano.function(inputs=[x,y], outputs=pred)\n",
    "\n",
    "    for epochidx in xrange(max_epochs):\n",
    "        for batchidx in iterator:\n",
    "            x, y, n_ex = getBatch(iterator)\n",
    "            cost = f_grad_shared(x, y)       # forward prop\n",
    "            f_update(lrate)                  # back prop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Frameworks\n",
    "**Lasagne** - lightweight library to build and train neural networks in Theano\n",
    "\n",
    "**blocks** - a Theano framework for building and training neural networks\n",
    "\n",
    "**keras**\n",
    "\n",
    "## Resources\n",
    "http://deeplearning.net/software/theano/tutorial/index.html#tutorial\n",
    "https://github.com/mila-udem/summerschool2015"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### talk about googlenet implementations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from lasagne import layers\n",
    "from lasagne.updates import nesterov_momentum\n",
    "from nolearn.lasagne import NeuralNet\n",
    "\n",
    "net1 = NeuralNet(\n",
    "    layers=[  # three layers: one hidden layer\n",
    "        ('input', layers.InputLayer),\n",
    "        ('hidden', layers.DenseLayer),\n",
    "        ('output', layers.DenseLayer),\n",
    "        ],\n",
    "    # layer parameters:\n",
    "    input_shape=(None, 9216),  # 96x96 input pixels per batch\n",
    "    hidden_num_units=100,  # number of units in hidden layer\n",
    "    output_nonlinearity=None,  # output layer uses identity function\n",
    "    output_num_units=30,  # 30 target values\n",
    "\n",
    "    # optimization method:\n",
    "    update=nesterov_momentum,\n",
    "    update_learning_rate=0.01,\n",
    "    update_momentum=0.9,\n",
    "\n",
    "    regression=True,  # flag to indicate we're dealing with regression problem\n",
    "    max_epochs=400,  # we want to train this many epochs\n",
    "    verbose=1,\n",
    "    )\n",
    "\n",
    "X, y = load()\n",
    "net1.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "net2 = NeuralNet(\n",
    "    layers=[\n",
    "        ('input', layers.InputLayer),\n",
    "        ('conv1', layers.Conv2DLayer),\n",
    "        ('pool1', layers.MaxPool2DLayer),\n",
    "        ('conv2', layers.Conv2DLayer),\n",
    "        ('pool2', layers.MaxPool2DLayer),\n",
    "        ('conv3', layers.Conv2DLayer),\n",
    "        ('pool3', layers.MaxPool2DLayer),\n",
    "        ('hidden4', layers.DenseLayer),\n",
    "        ('hidden5', layers.DenseLayer),\n",
    "        ('output', layers.DenseLayer),\n",
    "        ],\n",
    "    input_shape=(None, 1, 96, 96),\n",
    "    conv1_num_filters=32, conv1_filter_size=(3, 3), pool1_pool_size=(2, 2),\n",
    "    conv2_num_filters=64, conv2_filter_size=(2, 2), pool2_pool_size=(2, 2),\n",
    "    conv3_num_filters=128, conv3_filter_size=(2, 2), pool3_pool_size=(2, 2),\n",
    "    hidden4_num_units=500, hidden5_num_units=500,\n",
    "    output_num_units=30, output_nonlinearity=None,\n",
    "\n",
    "    update_learning_rate=0.01,\n",
    "    update_momentum=0.9,\n",
    "\n",
    "    regression=True,\n",
    "    max_epochs=1000,\n",
    "    verbose=1,\n",
    "    )\n",
    "\n",
    "X, y = load2d()  # load 2-d data\n",
    "net2.fit(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Debugging"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
